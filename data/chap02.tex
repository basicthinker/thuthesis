\chapter{研究现状}
\label{chap:related}

本章概述文件系统、事务性系统和软件透明等接口方式下，传统持久性内存系统的故障时数据一致性保证机制。前人工作大部分针对内存和存储二元结构设计，通常不直接适用于持久性内存系统，或者无法将持久性内存的性能优势完全发挥出来。在对比中，我们同时阐述了本文工作的优势。

\section{文件系统中的数据一致性机制}

\subsection{重新审视\texttt{fsync}系统调用}

作为POSIX标准的文件系统接口之一的\texttt{fsync}系统调用，在维系数据\emph{一致性}方面扮演着重要角色；而这主要因为它向调用者提供了数据\emph{持久性}的保证。然而一致性和持久性的紧耦合无法达到性能的苛刻需求。为此，若干工作探索了改变\texttt{fsync}语义，将两者分离的方式。

xsyncfs\cite{Nightingale:2006:RS:1298455.1298457}在保证数据一致性的前提下允许\texttt{fsync}返回以提高应用的执行速度，但为了使这种行为的改变对系统外部用户或者其他系统保持透明，xsyncfs缓冲所有用户可见的输出，直到\texttt{fsync}已经完成数据的持久化之后再释放这些输出。我们的工作着眼于更加特定的移动系统环境，给出的持久性保证基于新的持久性内存系统假设，从而进一步简化一致性相关设计，并针对移动环境采用比xsyncfs更加偏向能耗和响应度的折衷。

OptFS\cite{Chidambaram:2013:OCC:2517349.2522726}引入了
\texttt{osync}和\texttt{dsync}两个接口。前者能够支持传统的一致性需求，但仅确保\emph{最终}持久性，即调用结束时并不保证持久性。该模型为本文工作提供了重要参考。然而，OptFS的机制通过校验和来确保磁盘上写日志的一致性，并不适用于我们选取的应用目标，移动系统环境。更重要的是，该工作缺乏系统的定量的策略设计。其他相近工作\cite{Ma:2011:LPF:1989323.1989325,
Mickens:2014:BFC:2616448.2616473, Ports:2010:TCA:1924943.1924963}简单地使用一个静态的时间阈值来限制数据滞后性，无法自适应于不同的应用和用户。上述相关工作没有面向持久性内存和移动系统的设计，更没有利用两者的特殊性质。

\subsection{传统内存数据管理}

各种主存数据库系统\cite{DeBrabant:2013:ANA:2556549.2556575,DeWitt:1984:ITM:602259.602261,
Kallman:2008:HHD:1454159.1454211, Ongaro:2011:FCR:2043556.2043560}、动态的日志机制（adaptive logging）\cite{Kim:2010:ALM:1920841.1921023}、可恢复的虚拟内存系统（recoverable virtual memory）\cite{Satyanarayanan:1993:LRV:168619.168631}、面向闪存\cite{Dai:2004:EEL:1031495.1031516,
Kim:2012:GBC:2254756.2254786, Lv:2011:OBM:1989323.1989326}以及基于NVM\cite{Coburn:2013:AMT:2517349.2522724, hitz1994file, 6880383, Wu:1994:ENM:195473.195506}的存储系统都在着力优化内存数据刷出或回写到持久化介质的效率。基于NVM的换入换出机制\cite{6986137}显示了相较于传统设计更优的性能。我们同样关注该优化点，但这些工作都没有面向移动系统和持久性内存，无法提供我们目标环境下的最优解决方案。

qNVRAM\cite{183627}实现了一个持久性的页缓存，但要求使用新的API去访问该缓存，对遗留代码不友好。外部日志（external journaling）\cite{Jeong:2013:ISO:2535461.2535499}要求额外的存储设备，且未涉及能耗方面的优化。Fjord\cite{6558430}虽然做到区分应用进行特异性优化，但区分的方法仅根据应用是否基于云服务，进而依据此更改软件系统的配置。主机端闪存缓存（host-side flash caching）\cite{koller2013write}实行了类似的在系统性能和数据滞后性之间的折衷。与之前评述的其他工作类似，这些工作没有做到对移动环境以及不同应用或用户行为的适应性。我们的工作保持对\texttt{fsync}系统调用和页缓存的最小更改，适用于受限的移动环境；对折衷策略的定量化研究使得我们可以实现应用或用户特定的自适应的系统优化。

\subsection{能量和应用响应度优化}
BlueFS\cite{Nightingale:2004:ESF:1251254.1251279}在多个节点之间智能地选择代价最小的副本来服务用户请求。SmartStorage\cite{Nguyen:2013:SSE:2493432.2493505}牺牲了4\%-6\%的性能来换取能量效率，主要的手段是通过优化存储系统参数。而本文工作的目标是通过基于持久性内存的设计显著提高性能的同时达到节能的效果。Capsule\cite{Mathur:2006:CEO:1182807.1182827} 仅仅考虑随机的和顺序的数据访问特性来优化存储。本文的工作将对持久性内存条件下应用的访问模式和相应的优化策略给予定量的系统的研究。SmartIO\cite{Nguyen:2014:ISR:2638728.2638841}主要优化手段是设置读操作的优先级高于写操作。Mobius\cite{Chun:2012:MUM:2307636.2307650}则考虑了分布式环境下节点的位置、网络拥塞等要素。此外，通过增加I/O突发度（burstiness）提高能量效率的工作\cite{Papa:2003:EET, Weissel:2002:CIN:844128.844140}和本文的工作类似，但我们同时考虑动态的策略和异步\texttt{fsync}系统调用等手段。Simba\cite{188466}设计了一个同时面向本地存储和云存储的同步（sync）接口。与Simba类似，我们提供对文件系统和其上数据库系统的数据一致性保证。总之，我们面向移动系统的优化目标、对于系统行为的观察和多目标策略设计等有别于如上各类现有的系统优化工作。

\subsection{最新的移动文件系统}
F2FS\cite{188454}（在Moto X设备上测试）可以获得比Ext4\cite{MOTOX:2013}高128\%的随机写吞吐率。DFS\cite{Josephson:2010:DFS:1855511.1855518}通过将存储管理功能委托给闪存硬件来提升I/O性能。相比之下，我们面向持久性内存的解决方案实现的读写性能比现有文件系统设计高出几个数量级（本文测评结果可参见图~\ref{fig:bench-item}）。

\section{事务系统中的数据一致性机制}

相对于随机和顺序读写性能不对称的传统机械硬盘，人们尝试新的闪存和非易失性随机访问内存（NVRAM）等存储介质，更加高效地完成对传统“ACID”四属性中数据持久性（即“D”，durability）的支持。这种持久性系统的设计和优化，与事务系统的实现及其一致性机制紧密相关。数据库和事务性内存是最为典型的事务系统，原有一致性机制非常成熟，本节重点讨论在新的存储介质上持久化数据对数据一致性带来的挑战及其应对手段。

\subsection{在数据库中使用闪存}

数据库对表和索引的写入主要是随机的，而对事务日志、历史版本和临时表的写是顺序的。
下面两篇论文分别研究了通过应用固态硬盘（SSD）更好地支持这两类写的问题。页内日志（IPL）\cite{Lee:2007:DFD:1247480.1247488}在内存中的扩展页中缓存细粒度的数据更新，而后在智能选择的闪存位置上成批刷出这些缓存。该设计减少了写带宽，支持高速的从日志恢复页的操作，而且通过不覆写闪存数据避开了低效的写前抹除（erase-before-write）。然而，这些缓存的更新并不是\emph{同步地}实现持久化，而是需要全系统范围的一个额外的日志机制来保证其持久性。另一篇文献\cite{Lee:2008:CFM:1376616.1376723}是较早指出现有数据库中顺序写以及固态硬盘适用性的工作。特别地，针对事务日志，该工作比较了机械硬盘和固态硬盘在传统组提交机制下的表现。该工作指出了大量并发用户造成的事务吞吐率降低的问题，但仅仅将之归结于CPU的速度瓶颈。

进一步地，文献\cite{Lee:2009:AFM:1559845.1559937}和文献\cite{Chen:2009:FEF:1559845.1559855}展示了SSD的随机写性能可以胜任商用数据库的需求。他们指出，一块企业级闪存盘在事务吞吐率、性价比和能耗等方面，可以超过一个由8块每分钟15000转的企业级机械硬盘构成的RAID 0阵列。不过这些工作依然使用一个集中的较大的日志缓冲区（16 MiB）。文献\cite{Chen:2009:FEF:1559845.1559855}只利用了多个USB闪存设备来刷出日志缓冲区。而新的NVMe接口和SSD提供了更为丰富的并行性，可以进一步提升系统性能。

Aether\cite{Johnson:2010:ASA:1920841.1920928}是一种具有较强扩展性的日志机制。该论文分析了在高可扩展性数据库系统Shore-MT中预写式日志（write-ahead logging）的四个性能瓶颈：（1） I/O延迟，（2）日志导致的锁竞争（持有写锁），（3）上下文切换，以及（4）日志缓冲区的竞争。Aether通过提前释放锁和刷出流水线等方法将I/O延迟移出关键路径，并通过在数组中收集请求来消除请求对日志缓冲区的竞争。然而，这些数组最终依然会竞争日志缓冲区。基于Aether的文献\cite{raey}着力解决多插座（multisocket）的性能开销问题，主要手段是设计了分布式日志缓冲区。然而，他们的结论是其他跨插座通讯会抵消分布式日志的好处。

文献\cite{Pandis:2010:DTE:1920841.1920959}通过将每个工作线程对应于不重叠的数据子集来减少线程竞争。与将事务分配到工作线程的方式不同，该工作将事务分为若干子部分，然后依照数据分布将它们分配到各工作线程。这种设计选择，和早期基于阶段（stage-based）的服务器设计\cite{Welsh:2001:SAW:502034.502057}和基于线程的服务器设计\cite{vonBehren:2003:CST:945445.945471}之间的争论有相似之处。

在SiloR\cite{Zheng:2014:FDF:2685048.2685085}中，工作线程将日志项传递给专门的日志线程，后者将数据并行地刷出并维护日志项的顺序。我们的工作可以不使用额外的日志线程，仍然由工作线程完成数据刷出的任务。我们同时将SSD的性能特征考虑在内。

\subsection{在数据库中使用非易失性随机访问内存}

J. Huang等的工作\cite{Huang:2014:NLT:2735496.2735502}主要解决了集中式日志缓冲器的瓶颈问题。该工作提出每事务日志技术（per-transaction logging），并使用指针来跟踪日志项。这种设计依赖于非易失性随机访问内存的随机访问特性。文献\cite{Wang:2014:SLT:2732951.2732960}将NVRAM应用于分布式日志。但是该工作依然涉及跨日志的条目间顺序协调问题。本文工作以缓冲区为单位的顺序协调机制将简化该类设计。

R. Fang等的工作\cite{5767918}较早提出将写日志记入NVRAM构成的缓冲区，以避免传统组提交机制中延迟长的问题。但该设计依赖于NVRAM按字节访问的属性。

文献\cite{Arulraj:2015:LTS:2723372.2749441}探索了三种存储引擎架构下使用NVRAM的好处：本地更新（in-place update），写时复制更新（copy-on-write update），以及日志结构更新（log-structured update）。但他们底层依赖的预写式日志机制（write-ahead logging）依然是传统的集中式设计。

\subsection{在事务性内存中使用NVRAM}

以下工作与NVRAM按字节访问的特性紧密结合，大部分设计并不适用于SSD的接口。

Mnemosyne\cite{Volos:2011:MLP:1950365.1950379}使用原始字日志（raw word log）来实现预写式重做日志。每个原子性写入的字由一个反转比特（tornbit）标志是否完成。而NV-heaps\cite{Coburn:2011:NMP:1950365.1950380}则使用一个撤销日志（undo log）。文献\cite{Kannan:2014:RCP}提供一个灵活的基于对象或字的日志接口。文献\cite{Pelley:2014:MP}定义了多种模型来强制NVRAM写持久化的顺序。Atlas\cite{Chakrabarti:2014:ALL:2660193.2660224}不依赖于事务性内存，但可以将日志机制扩展到基于锁的程序。我们的工作也可以借助Atlas，将SSD的持久化能力提供给靠锁保护的常规虚拟内存访问。

\subsection{在事务性内存中使用SSD}

这部分工作与本文提出的解决方案较为类似。

Hathi\cite{Saxena:2012:HDT:2236584.2236589}支持内存中的ACID事务，并且使用SSD作为最终持久化手段。它主要从两个方面改进了常规预写式日志：首先，异步的提交机制，程序可以在提交后查询持久化是否完成；其次，每个线程本地的日志可以避免对集中式日志缓冲区的争抢。然而，异步提交机制将查询负担转嫁给了应用程序，而线程本地的日志依然需要协调各个线程来维持全局递增的序列号（log sequence number，LSN）。整体上，该工作在实验测评中的表现最佳的事务吞吐率可以达到完全异步提交的13\% ，而代价是允许持久化有0.2~ms的延后。

Stasis\cite{Sears:2006:SFT:1298455.1298459}尝试通过事务而不是数据库系统，将应用需求和底层存储硬件连接起来。该工作重点在于高效和灵活的接口设计，其内部依然使用了传统的预写式日志的设计。

Violet\cite{Santry:2014:VSS:2643634.2643637}认为将内存中的图结构映射到数据库或者对传统磁盘友好的存储格式都是低效的。所以，它使用事务性内存来收集应用对数据的更改，然后使用斐波那契数组（Fibonacci Array）将这些更改保存到块存储设备中。值得注意的是，限于Intel的事务性内存模型，成功执行的事务所做的修改在完全持久化之前即可以被其他事务看到。

\subsection{其他}

本节介绍的文献主要在整个软件系统栈的其他层开展工作。它们或者印证了我们对系统架构的选择，或者支持着我们采纳的系统假设。

对象存储也是一类接口方式。例如，HEAPO\cite{Hwang:2014:HHP:2705611.2629619}是一个基于锁的键值存储（key-value store），构建在一个基于NVRAM的对象存储系统上。该键值存储使用撤销日志应对故障时数据一致性问题。作者指出，他们使用的日志技术的一个优点在于可以减少事务性内存写入非易失性内存的数据量。然而，如果将此类技术作为使用持久性内存的通用手段，其显著的缺点是程序员被限制在使用该键值存储特定的接口和实现上。
此外，Tango\cite{Balakrishnan:2013:TDD:2517349.2522732}设计了分布式服务来支持从多个客户端对虚拟内存中对象的事务性访问。它依靠一个共享的持久性日志来维护事务性语义。而本文工作将聚焦于单机优化。

文献\cite{Huang:2015:UAT:2749469.2750420}通过将虚拟内存、文件系统和闪存转换层（FTL）的地址转换、状态和权限检查统一起来，提升系统整体的I/O性能。从某种意义上说，我们的工作也包含了相近的设计思想，即简化和贯穿传统的厚重的I/O软件栈。

先前的测评类工作\cite{Stoica:2009:ERW:1565694.1565697,Chen:2009:UIC:1555349.1555371,Jung:2013:RWH:2465529.2465548}为我们理解SSD特性和解决方案设计奠定了基础。特别地，文献
\cite{Stoica:2009:ERW:1565694.1565697}指出了这样一个现象：在一个长达24小时的测试中，SSD的随机写性能会随着时间推移逐步降低。所以我们的设计会谨慎地选择随机写，而主要采用小的顺序写来完成持久化功能。

Moneta\cite{Caulfield:2010:MHS:1934902.1934984}是一个面向非易失性内存的存储阵列。它的系统设计通过避免不必要的锁、中断和调度来降低软件带来的开销（测评显示，通过PCIe访问基于NVRAM存储的传统文件系统软件，会占用一个4~KB写操作延迟中的68\%）。在最优化的实现下，我们依然可以看到顺序写（单个写大小8~KB）能够达到3倍于随机写（单个写大小0.5~KB)的吞吐率。而另一件工作\cite{Kang:2014:DWC:2588555.2595632}，在SSD中使用持久性缓存以提高读写性能，但受限于存储介质能力和软件接口，其随机写吞吐依然低于65~MB/s。这些和我们对当前基于闪存的SSD产品的测评结果相符，说明我们系统设计假设是广泛成立的。

此外的一些软件优化技术\cite{Yu:2014:OBI:2642648.2619092}，如轮询I/O的完成状态、消除I/O路径上的上下文切换、合并不连续的请求、面向SSD特性配置的调度器、解决预读问题和避免请求队列争抢锁等，与本工作是正交的，而且很多已经为当前NVM Express接口设计所融合。

\section{软件透明的数据一致性机制}

据我们所知，本文工作首次提出了软件透明地支持运行无修改的、遗留应用的持久性内存设计，该设计不给程序员带来特别的编程负担，亦不要求特殊的电源管理。大部分其他的持久性内存设计\cite{Volos:2011:MLP:1950365.1950379,
Coburn:2011:NMP:1950365.1950380,
Venkataraman:2011:CDD:1960475.1960480, Intel:PMEM, SNIA:2013:NPM,
Zhao:2013:KCP:2540708.2540744, Moraru:2013:CDS, Kannan:2014:RCP,
Liu:2014:NDU:2541940.2541957, Pelley:2014:MP}均要求程序员使用新的为操作持久性内存上的数据而开发的APIs，实现新的程序或者重写遗留代码。

\subsection{基于软件的持久性内存设计中的API}
持久性内存在软件社区中得到了充分的关注和研究\cite{Volos:2011:MLP:1950365.1950379,
Coburn:2011:NMP:1950365.1950380,
Venkataraman:2011:CDD:1960475.1960480, Intel:PMEM, SNIA:2013:NPM,
Shapiro:1999:EFC:319151.319163, Chen:1996:RFC:237090.237154}。大部分前人工作都开发了应用级别或者系统级别的接口来提供故障时数据一致性的保证。例如，Mnemosyne\cite{Volos:2011:MLP:1950365.1950379}和NV-heaps\cite{Coburn:2011:NMP:1950365.1950380}采用了应用级别的编程接口用于管理持久性内存区域的数据；程序员们需要显式地声明、分配和回收持久性对象。CDDS（consistent and durable data structures）\cite{Venkataraman:2011:CDD:1960475.1960480}要求应用开发人员修改已有的数据结构以实现高效的对持久性数据的更新。EROS\cite{Shapiro:1999:EFC:319151.319163}和Rio文件缓存\cite{Chen:1996:RFC:237090.237154}需要系统开发人员重新实现操作系统组件来实现高效的数据持久化。Intel\cite{Intel:PMEM}和SNIA\cite{SNIA:2013:NPM}引入的新的应用程序编程接口要求程序员显式地声明持久性对象\cite{Volos:2011:MLP:1950365.1950379,
Coburn:2011:NMP:1950365.1950380}、重新实现内存中的数据结构\cite{Venkataraman:2011:CDD:1960475.1960480}或者修改遗留代码来使用事务接口\cite{Volos:2011:MLP:1950365.1950379,Coburn:2011:NMP:1950365.1950380}。

\subsection{基于硬件的持久性内存设计中的API}
对持久性内存的硬件支持得到日益增加的关注\cite{meza2013case, Condit:2009:BIT:1629575.1629589,
Zhao:2013:KCP:2540708.2540744, Moraru:2013:CDS, Pelley:2013:SMN,
Kannan:2014:RCP, Liu:2014:NDU:2541940.2541957, Pelley:2014:MP,
6378661, 6974684, 7011385, justin-taco14}。这些工作大部分采用了与基于软件的持久性内存系统开发的API相似的接口。例如，Kiln\cite{Zhao:2013:KCP:2540708.2540744}使用一个基于事务的API来获得来自软件的对于关键数据的更新。Moraru等\cite{Moraru:2013:CDS}提出的设计要求程序员显式分配持久性对象。由Pelley等\cite{Pelley:2014:MP}提出的松弛持久化模型采用新的API来支持他们特殊的持久化模型。BPFS\cite{Condit:2009:BIT:1629575.1629589}在文件系统中使用非易失性内存，会带来可观的软件性能开销。

\subsection{通过特殊电源管理实现的故障时数据一致性}
全系统持久化（whole-system persistence）\cite{Narayanan:2012:WP:2150976.2151018}可以提供不增加程序员负担的故障时数据一致性，但依赖系统电源提供的残余电量（residual energy）在系统故障时刷出CPU的缓存。该模型受限于残余电量可以刷出的数据大小，所以无法应用于包含DRAM的混合内存系统。自动提交内存（auto-commit memory）\cite{flynn2012apparatus}使用二级电源供应，在系统失效时，将易失的数据提交到闪存中。这些额外的电源供应增加了整个系统的成本、维护难度和潜在的不可靠性。


% Moraru
% \emph{et al.}\cite{Moraru:2013:CDS} developed a cache-efficient
% consistency-preserving data update scheme; however, it requires programmers to
% explicitly allocate persistent objects with a NVM malloc function. Pelley
% \emph{et al.}\cite{Pelley:2014:MP} introduces a relaxed persistence model to
% minimize the ordering control overhead and enable coalesce of writes to the same
% data; however, it employs a strand API to support their proposed strand
% persistence model. %\vspace{0.05in}

本文工作不要求任何应用或系统层面的新的API。所以它提供了一种让用户不修改程序即可利用持久性内存的有效方法。

% Two prior studies, whole system
% persitence~(WSP)\cite{Narayanan:2012:WP:2150976.2151018} and auto-commit
% memory~(ACM)\cite{Ouyang:2011:BBI, flynn2012apparatus} support transparent
% persitsence management. However, WSP requires additional interrupt handlers to
% perform ``flush-on-fail'' in the event of system failures. Furthermore, the
% design needs to incorporate additional hardware components, including a
% prorammable microcontroller to monitor the status of power supply and a super
% capacitor used as backup voltage supply when system loses power. THNVM does not
% introduce any of the software interrupts and off-chip hardware components, and
% therefore is a more lightweight implementation compared with whole-system
% persistence. ACM exposes a designated DRAM region to CPU and commits updates to
% that region to flash automatically in background. An auto-commit buffer is used
% to buffer updates, but will become a bottleneck for memory-intensive workloads.
% No mechanism is proposed to handle the situation when the flash throughput
% cannot catch up with memory writes. In contrast, THNVM can overlap checkpointing
% with program execution in a similar situation, and therefore can offer much
% higher system performance.

%% NVM Duet\cite{Liu:2014:NDU:2541940.2541957}
%% relaxes consistency and durability guarantees for persistent store to achieve
%% high performance in a non-volatile memory system shared by working memory and
%% persistent memory.

\subsection{检查点生成及其粒度选择}
检查点生成是一种在不同领域广泛应用的技术\cite{466999, 5645453}。例如，ReVive\cite{1003567}、SafetyNet\cite{1003568}、
Mona\cite{Gao:2015:RIC}以及COMA\cite{1563035}的检查点生成机制均基于CPU缓存块粒度（细粒度）生成内存数据的检查点，这些工作的应用目标是提高数据的可用性。另一些工作\cite{6468462, Oliner:2006:CCR:1183401.1183406,
Rajachandrasekar:2014:MDC:2600212.2600713}则采用粗粒度来生成检查点。相比于它们，我们的机制采用两种不同的粒度来形成一个检查点，获得多种粒度的好处。据我们所知，本文工作首次提出根据访问模式的局部性调整检查点生成的粒度，从而实现高性能和低开销。另外，闪存转换层（flash translation layer）可以利用两种粒度\cite{Kang:2006:SFT:1176887.1176911,
Lee:2007:LBF:1275986.1275990}但是不提供数据一致性保证，且均特定地面向闪存。

