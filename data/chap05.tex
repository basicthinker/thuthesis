\chapter{双模式检查点生成技术}
\label{chap:thynvm}

新兴的字节编址的（byte-addressable）非易失性内存介质，诸如STT-RAM~\cite{4443191}、PCM~\cite{Raoux:2008:PRA}和ReRAM~\cite{5607274}等预示着持久化内存系统，一个在内存和存储栈上的新的中间层。持久化内存系统融合了传统内存系统（快速的load/store指令接口）和存储系统（数据持久化）的优良特性，模糊了两者之间的界限。它给应用带来的一个重要的益处是，可以通过load/store指令高效地直接地访问内存中的持久化数据，而不需要与存储设备进出换页、在（反）序列化时更改数据格式以及触发臃肿的系统调用。

持久化内存相对于传统的易失性内存系统引入了一个关键的要求，系统故障时的一致性保证~\cite{Lamb:1991:ODS,Copeland:1989:CSR,Shapiro:1999:EFC:319151.319163}。这要求持久化内存系统，在掉电或系统失效等故障出现时，依然能够保证数据的一致性不受部分或是乱序的NVM写的影响。我们以原子性地更新保存在NVM上的数据结构A和B为例，总会有对A或B的一个更改会先于其他写入NVM。如果系统在一个更改完成之后出现故障或掉电，两个数据结构可能处于不一致的中间状态，只包含部分更改。对于传统易失性内存，这不是一个问题，因为程序恢复后内存中的数据都会丢失。但对于持久化内存，这些不一致的数据会一直保留。所以，持久化内存系统需要保证保存到NVM的数据可以在系统重启后恢复到一个一致的状态，即维持数据的一致性。维持数据的一致性原本仅是对磁盘或闪存等存储系统的要求，但将数据持久化引入内存后，它即成为内存系统同样面临的一个挑战。

大多数之前的持久化内存设计依赖于程序员的人工努力来保证系统故障时的数据一致性。应用的开发人员需要显式地使用特定的编程模型和软件接口来访问和操作内存中的持久化数据。这些软件接口构建于为持久化内存定制的运行时库~\cite{Condit:2009:BIT:1629575.1629589,Volos:2011:MLP:1950365.1950379}或硬件架构~\cite{Zhao:2013:KCP:2540708.2540744}。这种设计似乎给程序员提供了对数据持久化的完全控制，但要求程序员管理持久化内存会带来至少三方面不良影响。首先，应用开发者必须用新的编程接口API来实现程序或者深度更改遗留代码，通常要显式地声明和划分持久的和临时数据结构。其次，大多数之前的持久化内存设计需要使用事务性内存控制版本和对NVM写的顺序，而事务性内存的扩展性依然面临挑战。第三，现在持久性内存的编程模型可能基于很多种语义和软件接口实现，会带来兼容性和移植性问题。为此，我们工作的目标是设计高效的故障时数据一致性保障机制，使得持久化内存的使用范围得到扩展而不必需要复杂的应用更改。

我们提出了ThyNVM，一种新的支持对软件透明的故障时数据一致性的持久化内存设计。它允许基于事务的和基于锁的程序直接在持久化内存硬件上运行。ThyNVM使用DRAM和NVM混合的架构和高效的硬件辅助的检查点生成技术来达到DRAM水平的性能。这里的一个关键挑战是减少由于生成检查点而导致的应用停滞时间。我们发现，在停滞时间和元数据存储开销之间存在一个折衷。在较小的粒度上生成检查点，带来的停滞时间可以比较短，而对应元数据占用的空间却会非常大；在较大的粒度上生成检查点，可以降低元数据占用的空间，但是会导致较长的停滞时间。因此，单一的检查点生成技术（或者基于小粒度或者基于大粒度）难以达到最优的效果。为了解决这一问题，我们提出了双模式检查点生成机制，它可以同步地为分散的和集中的内存写分别基于CPU缓存块粒度和操作系统页粒度生成检查点。与先前使用写时拷贝（copy-on-write，COW）或日志技术的持久化内存设计相比，ThyNVM显著减小了存储的损耗并增加了内存带宽的利用率。

特别地，本章工作做出了如下几点贡献：（1）我们提出了一种新的持久化内存设计，支持对软件透明的故障时数据一致性保障。我们的设计允许基于事务的和基于锁的应用通过load/store指令接口使用持久化内存。（2）对于生成检查点的粒度，我们定位了程序停滞时间和元数据空间占用之间的权衡关系。在任意一个粒度上追踪对持久化内存的更改都不是最优的。（3）我们设计了一个新的双模式检查点生成技术。我们的技术比页粒度的检查点生成机制减少了86.2\%的停滞时间，而仅比缓存块粒度的检查点生成机制多用26\%的内存控制器硬件空间。（4）我们在内存模拟器上实现了双模式检查点生成机制，使用我们经过形式化证明的一致性协议。它可以在保证数据一致性的同时，将检查点生成过程和程序执行重叠。我们的解决方案可以达到不提供一致性支持的全DRAM系统性能的95.1\%。

\section{软件透明的一致性保证机制}

ThyNVM的设计有两个本质特性，软件透明和检查点生成。本节讨论我们开发这两个特性的原因。

\subsection{软件一致性保障技术的缺陷}

\begin{figure}[!h]
\centering \includegraphics[width=0.9\linewidth]{motivation-code}
  \caption{实例代码：向持久化的哈希表中插入一个项。分别采用（a）传统的软件接口或（b）软件透明的ThyNVM。}
\label{fig:motivation-code}
\end{figure}

之前大多数持久化内存设计~\cite{Condit:2009:BIT:1629575.1629589,Volos:2011:MLP:1950365.1950379,Coburn:2011:NMP:1950365.1950380,Zhao:2013:KCP:2540708.2540744,Venkataraman:2011:CDD:1960475.1960480}要求应用开发者显式地定义持久性数据并使用特定库来访问数据——故障时数据一致性保障机制与软件的语义紧耦合。我们通过图\ref{fig:motivation-code}中的实例代码来展示这种持久化内存设计的不足。该图比较了一个更新持久性哈希表项的软件方法的两种实现（代码改编自STAMP~\cite{Cao:2008:STA}）。图\ref{fig:motivation-code}（a）展示了使用类似于现有设计~\cite{Condit:2009:BIT:1629575.1629589, Volos:2011:MLP:1950365.1950379}的软件事务的实现；而图\ref{fig:motivation-code}（b）中的实例代码采用了ThyNVM的软件透明的故障时数据一致性支持。我们可以看到，在支持数据一致性时涉及软件支持，会有多方面的不足。

首先，划分临时性和持久性数据很麻烦而且容易出错。图\ref{fig:motivation-code}（a）展示了许多无法避免的标记。程序员需要对软件行为有深入的了解，并且小心地确定哪些数据结构需要持久化以及如何操控这些数据。例如，图\ref{fig:motivation-code}（a）的第3行仅当这个哈希表实现固定数目的桶时才是正确的。

其次，在一个统一的地址空间内，临时性和持久性数据之间的引用需要小心的管理。然而，对该问题的基于软件的解决方案并非最佳。例如，为了处理NV-to-V指针（非易失性指针指向易失性数据，图\ref{fig:motivation-code}（a）的第8行），NV-heaps~\cite{Coburn:2011:NMP:1950365.1950380}会简单地抛出一个运行时错误以禁用此类指针。

第三，应用需要使用事务，通常是通过事务性内存接口（例如被Mnemosyne~\cite{Volos:2011:MLP:1950365.1950379}使用的TinySTM~\cite{Felber:2008:DPT}），来更新持久化数据，如图\ref{fig:motivation-code}（a）所示。遗憾的是，事务性内存，不论基于硬件还是基于软件，存在多方面的可扩展性问题~\cite{Cascaval:2008:STM:1454456.1454466,Pankratius:2011:STM:1989493.1989500,Dice:2009:EEC:1508244.1508263}。此外，开发人员必须使用新的API来实现或者重写各种代码库和程序，需要不可忽略的人工投入。不熟悉这些新API的程序员需要一个痛苦的学习过程。

最后，应用需要构建在特定的库之上，例如libmnemosyne~\cite{Volos:2011:MLP:1950365.1950379}和
NV-heaps~\cite{Coburn:2011:NMP:1950365.1950380}。这会很大程度上降低持久化内存应用的移植性——在一个库上实现的应用，如果想采用其他的库，需要重新实现。编译器确实可以在一定程度上减轻程序员实现和移植代码的负担。然而，编译器会无差别地插装所有内存的读和写操作，带来可见的性能损耗。我们在GCC libitm~\cite{libitm}上实验了STAMP事务性标准测试~\cite{Cao:2008:STA}，结果插装本身最多可带来高达8\%的性能损失。

我们采用了允许软件程序直接访问持久化内存的使用模型。如图\ref{fig:motivation-code}（b）所示，我们可以不更改代码而实现数据结构的持久化。

\subsection{日志及写时拷贝技术的缺陷}

前人基于软件的持久化内存设计，大部分通过修改现有软件事务性内存库、文件系统或者数据库实现，采用日志~\cite{Volos:2011:MLP:1950365.1950379, Coburn:2011:NMP:1950365.1950380}或者写时拷贝技术~\cite{Condit:2009:BIT:1629575.1629589,
Venkataraman:2011:CDD:1960475.1960480}维持数据一致性。然而，两种技术为支持数据一致性保障会引入较大的性能损耗。

日志可能消耗比原始数据大很多的NVM空间，因为每个日志项都是一个既包含数据又包含元数据（例如数据所在的地址）的元组，而且通常每次写操作都需要记录~\cite{Volos:2011:MLP:1950365.1950379,
Coburn:2011:NMP:1950365.1950380}\footnote{某些对于日志的改进或变形~\cite{1003568}使用一个索引结构来合并更新。该设计即与我们双模式之一类似。}。此外，日志重放增加了故障后系统恢复时间，会抵消使用NVM而不是顺序访问的块设备带来的快速数据恢复的好处。

写时数据拷贝有两个缺点。首先，拷贝操作代价较大，会引发较长的停滞时间。其次，它不可避免地会复制未更改的数据，消耗额外的NVM带宽。这个缺陷在更新的地址很分散时尤为突出。

相比之下，生成检查点是一种更加灵活的方法，它定期地将易失性的数据及元数据写入NVM形成一致的内存数据的快照。我们仔细挑选和协调两种检查点生成模式，使它们同步地在不同粒度上工作，从而避免日志和写时拷贝的弊端。


\section{双模式检查点生成技术}

\subsection{粒度的权衡}

CPU缓存在块粒度上（典型配置为64字节）管理数据；操作系统在页粒度上（典型配置为4KB）管理数据。我们发现两个不同的粒度会带来一个在检查点生成停滞时间和元数据存储空间之间的折衷。

当我们选择在缓存块粒度上追踪持久化内存的更改时，各个细粒度写如果直接发送到NVM，可以在一两次总线传输的延迟内完成。这样，ThyNVM只需要在生成检查点时将追踪这些数据所用的元数据进行持久化即可。这些元数据可以保存在一个表中，每个表项保存一个从缓存块在内存中的地址到它实际检查点或工作数据位置的映射。这种模式的缺点在于元数据表项的数量是按页粒度追踪数据所需表项的64倍，因为每个页包含64个缓存块。相反地，如果我们在页粒度上追踪持久化内存的更改，我们可以极大地降低保存元数据表所需的硬件空间开销。然而把整页的数据写入NVM所造成的延迟，远大于更新一个缓存块。为了提升系统性能，ThyNVM使用DRAM来缓冲页粒度的更新，一段时间后再在NVM上生成这些页的检查点。尽管如此，将整页数据从DRAM传送到NVM依然会给系统带来较长的停滞时间。

考虑到如上的折衷，只在一个粒度上追踪持久化内存的更改不是最佳方案。这种情况下，我们需要开发一种双粒度的检查点生成技术。

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{arch}
\caption{ThyNVM的总体架构图。}
\label{fig-arch}
\end{figure}

ThyNVM采用DRAM和NVM的混合架构，基于硬件实现，提供软件透明的故障时数据一致性保障。ThyNVM以透明于应用程序的方式保障所有内存数据在故障时的一致性。为了利用在不同粒度持久化数据带来的不同特性，我们提出了一种新的双模式检查点生成机制。它可以同时在两个粒度上生成内存数据的检查点：分散的写在缓存块粒度上生成检查点，使用块重映射模式（第\ref{subsec:block-remap}节）；集中的写在页粒度上生成检查点，使用页回写模式（第\ref{subsec:page-writeback}节）。

另外，我们提出了两种机制来协调双模式检查点生成（第\ref{subsec:coordination}节）。第一，我们允许相对较快的块重映射模式来暂时接管原本应该由较慢的页回写模式处理的写。这样做可以显著减少检查点生成阻塞程序执行的时间。第二，我们定期地依据程序执行时访问内存的特点在两种模式之间迁移数据页。这样做可以有效保证检查点生成模式和其处理的写的行为相匹配。

图\ref{fig-arch}绘制了ThyNVM架构总览。DRAM和NVM部署在内存总线上，并映射到同一个物理内存地址空间中。该地址空间暴露给操作系统。我们更改了内存控制器，增加了一个检查点生成控制器，和两个用于维护内存访问元数据的地址转换表。这两个表分别是在缓存块粒度和页粒度管理元数据的块转换表（block translation table，BTT）和页转换表（page translation table，PTT）。

\subsection{系统定义}

\textbf{失效模型}：ThyNVM允许软件应用在诸如系统死机、掉电等故障发生后，从一个一致的内存数据检查点开始，继续CPU执行。为此，我们需要定期将内存中的数据的更新和CPU状态生成检查点，这其中包括CPU寄存器、写缓冲、脏缓存块以及内存控制器的写队列。我们的检查点生成机制保护内存数据和CPU状态的一致性不因系统故障而被污染。

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{epoch-before}\\
{\small (a) 采用检查点生成时暂停全系统的时间单元模型}
\includegraphics[width=0.9\linewidth]{epoch-after}\\
{\small (b) ThyNVM采用的时间单元模型，可将检查点生成和程序执行相互重叠。该模型维护三个版本的数据：活跃的工作数据（$W_{active}$）、最近检查点数据（$C_{last}$），和次近检查点数据（$C_{penult}$）。\hfill}
\caption{检查点生成系统的时间单元模型。}
\label{fig-epoch}
\end{figure}

\textbf{时间单元模型}：我们逻辑上把程序的执行时间分成连续的时间区段，称为时间单元（epoch，如图\ref{fig-epoch}）。每个时间单元包括一个执行阶段和一个检查点生成阶段。执行阶段更新工作数据，而检查点生成阶段持久化内存中的数据和CPU状态。

让执行阶段和检查点生成阶段顺次交替（如图\ref{fig-epoch}（a）所示）会导致显著的性能下降。对于内存访问频繁的负载，检查点生成可以消耗高达35.4\%的程序运行时间。所以，ThyNVM采用了一种将检查点生成阶段和程序执行阶段重叠的时间单元模型~\cite{1003568}，以降低该性能损失。我们定义三个连续的时间单元依次为活跃的、最近的和次近的时间单元。如图\ref{fig-epoch}（b）所示，一个新的活跃时间单元（时间单元2）可以在最近时间单元（时间单元1）的执行阶段和次近时间单元（时间单元0）的检查点生成阶段两者中更晚结束的一个之后即开始。

\textbf{数据版本}：执行阶段和检查点生成阶段重合允许两个相邻的时间单元同时访问相同的数据。这对数据一致性的维护带来了两个挑战。第一，相互重合的执行阶段和检查点生成阶段可能会覆盖对方的数据更改，所以我们需要隔离不同时间单元的数据更改；第二，系统失效发生时，活跃时间单元和最近时间单元的数据可能被同时损坏。为了应对这些挑战，ThyNVM维护了连续时间单元下数据的三个版本：\emph{活跃的工作副本 $W_{active}$}、\emph{最近检查点$C_{last}$}和\emph{次近检查点$C_{penult}$} （$C_{last}$和$C_{penult}$既包括内存数据也包括CPU状态）。如图\ref{fig-epoch}（b）所示，当我们在执行时间单元2（更新$W_{active}$）的时候，ThyNVM会同时保留时间单元0的检查点（$C_{penult}$）和时间单元1的检查点（$C_{last}$）；ThyNVM只有在时间单元1的生成检查点阶段结束后，才丢弃时间单元0产生的检查点。一个发生在时间$t$的系统故障，可能损坏时间单元2的工作数据副本以及时间单元1的检查点；此种状况下，我们可以依靠$C_{penult}$回滚到时间单元0。

\subsection{基于块重映射的检查点生成}
\label{subsec:block-remap}

ThyNVM对空间局部性低的数据更新使用块重映射模式进行管理。在每个时间单元的执行阶段，块重映射模式直接将工作数据写入NVM。因此，在相应的检查点生成阶段，ThyNVM可以简单地通过生成元数据（即BTT中的项）的检查点，将工作数据直接转为检查点数据。这样的方法可以大幅减少检查点生成的延迟。

为防止NVM中的更新损坏先前版本的数据，ThyNVM将新的缓存块的更改映射到一个其他的地址。ThyNVM将这类从原始地址到新地址的映射记录在BTT中。随后的内存读操作可以通过查找BTT来确定工作数据副本的有效位置。随后的以原始地址为目标的写操作，只要在同一个活跃时间单元内，可以进行归并，更新到该新地址。在下一个时间单元，该新地址的数据版本自然变为最近的检查点的一部分。NVM保存所有这些检查点数据；并且，ThyNVM在生成检查点阶段的开始会将BTT持久化到NVM。

\subsection{基于页回写的检查点生成}
\label{subsec:page-writeback}

页回写模式用于在页粒度上生成空间局部性较高的写操作的检查点。ThyNVM使用DRAM作为工作数据副本的缓冲。它首先在DRAM上归并内存的更新，然后在每个时间单元结束时生成检查点，将所有DRAM的脏页回写到NVM上。

这里的DRAM在两方面不同于CPU回写缓存或者简单的数据缓冲。第一，内存访问不会触发任何页置换或剔除；脏页只有在生成检查点时被回写。第二，当ThyNVM从DRAM回写一个脏页到NVM时，不可以直接覆盖NVM上原有的检查点数据。所以ThyNVM需要将每个脏页重定向到一个不同的地址上。ThyNVM使用PTT来为内存中的数据页追踪这些地址映射。在检查点生成结束的时候，PTT会被自动持久化到NVM上。这个标志着一个时间单元的结束。

\subsection{协调两种模式}
\label{subsec:coordination}

我们设计了两种机制来协调双模式，以达到如下两个目标：（1）减少程序执行时由于生成检查点带来的停滞时间；（2）调整检查点生成模式来适应动态变化的内存访问行为。

\textbf{两种模式的合作}：块重映射模式允许系统在检查点生成阶段仅持久化元数据。所以，它在检查点生成阶段能够比页回写模式更快地完成。这种情况下，页回写的检查点生成可能阻塞整个程序的执行，因为后续时间单元无法更改还未被写入NVM的DRAM页。为了缓解该问题，ThyNVM通过主动地借用块重映射模式来临时处理那些本该由页回写模式管理的新到的写请求，允许系统开启执行下一个时间单元。通过这种方式，我们隐藏了页回写检查点生成的阻塞时间，实现并行地写NVM和DRAM，并且提高了内存带宽的利用率。

\textbf{两种模式间数据迁移}：ThyNVM需要确保检查点生成采用的模式与数据不断变化的空间局部性和密集度特性相匹配。内存控制器在每个时间单元开始的时候，依据页上近期更改所呈现的空间局部性，决定一些页是否需要从一个模式切换到另一个模式。切换检查点生成模式涉及元数据更新和数据迁移。从页回写模式切换到块重映射模式，我们需要剔除PTT中的相应的项，同时将DRAM中页的可见工作数据移动到NVM上由块重映射模式管理的区域。这样，下次该页的任意缓存块被更改时，内存控制器就会自动将元数据信息添加至BTT中。从块重映射模式切换到页回写模式，我们需要在PTT加入一个项来存储页的元数据，同时将该页所有缓存块数据集中拷贝至DRAM中的相应位置。

\section{系统实现}
\label{sec:implementation}

本节描述内存地址空间管理和ThyNVM引入的硬件更改。我们也给出了ThyNVM操作读、写和检查点的实例，以及恢复由ThyNVM维护的数据的步骤。

\subsection{地址空间管理}
\label{subsec:thnvm-space}

ThyNVM地址空间管理的目标包括（1）在不同的内存位置存放数据的多个版本，以及（2）提供一个有效的一致的数据版本给操作系统。

\textbf{DRAM和NVM中的数据区域：}为了实现第一个目标，我们在DRAM和NVM中维护三个数据区域，每个保存一个数据版本：$W_{active}$、$C_{last}$或者 $C_{penult}$。如图\ref{fig-addr-space}所示，这些区域包括一个DRAM中的工作数据区和两个NVM中的检查点数据区。该工作数据区主要保存由页回写模式管理的活跃工作数据$W^{page}_{active}$，同时在上一个时间单元仍在生成检查点$C_{last}$时，用于缓冲由块重映射模式管理的活跃数据副本$W^{block}_{active}$。检查点区域A和B以交错的方式保存检查点数据$C_{last}$和$C_{penult}$：如果$C_{last}$保存在检查点区域A，我们就把$C_{penult}$写入检查点区域B；反之亦然。另外，如果上一个时间单元已经完成检查点生成，我们以与$C_{last}$交错的方式直接将$W^{block}_{active}$写入检查点区域（即用$W^{block}_{active}$覆写$C_{penult}$如果全内存$C_{last}$已经完整生成）。内存数据的大部分在三个连续时间单元（活跃、上一个和倒数第二个）都不会被更改，这些数据会位于检查点区域B，所以该区域也被称作家区域。另外，我们在检查点区域A开辟专门的内存区域来保存CPU寄存器的检查点。 

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{addr-space}
\caption{ThyNVM的地址空间布局}
\label{fig-addr-space}
\end{figure}

\textbf{物理地址空间到硬件地址空间的映射}。为了实现第二个目标，ThyNVM创建两个地址空间之间的映射关系。硬件地址空间由内存控制器管理，是DRAM和NVM实际的设备地址空间，仅对内存控制器可见。物理地址空间由操作系统管理，是对内存控制器对外提供的内存空间的逻辑视图，实际上包含硬件地址空间里数据的一个子集。我们定义\emph{可见数据}为在当前活跃时间单元内被更新的工作副本或者最近的检查点，即： 
\[
\begin{cases}
        W_{active}, & \text{如果}W_{active}\text{存在}\\
        C_{last}, & \text{如果}W_{active}\text{不存在}
\end{cases}
\]
在数据恢复的时候，我们复原如下的可见数据： 
\[
\begin{cases}
        C_{last}, & \text{如果上一个检查点生成完毕}\\
        C_{penult}, & \text{如果上一个检查点未完整保存}
\end{cases}
\]
我们设计物理地址空间的大小等于硬件地址空间中家区域的大小；也就是说，如果一个数据块或页保存在家区域中，那么它的物理地址等于它的硬件地址。任何没有在BTT/PTT中的数据块/页的可见数据都位于家区域中。

\subsection{元数据管理}

保存在BTT和PTT中的元数据为内存控制器提供必需的信息，以完成如下三个目标：（1）将处理器发出的每个内存请求的物理地址转换成硬件地址；（2）将生成检查点的NVM写指向正确的硬件地址；（3）确定何时在两个检查点生成模式之间迁移数据。ThyNVM在两个地址转换表BTT和PTT中维护元数据。连个表都包含四个列（图\ref{fig:metadata}），包含一个物理地址高位的块/页索引（BTT需要42比特，PTT需要36比特）、一个2比特的版本ID、一个2比特的可见内存区域ID、一个1比特的检查点内存区域ID和6比特的表征空间局部性的写计数器。 

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{metadata}
\caption{BTT和PTT表项的各个列。}
\label{fig:metadata}
\end{figure}

为了达到第一个目标，必要的信息包括：（a）一个请求应当指向的内存区域，以及（b）在该内存区域内物理地址到硬件地址的映射。处理器只能够访问可见数据，所以我们用可见内存区域ID确定对应可见数据所处的内存区域。为了获得硬件地址，我们确保每个表项到表首的偏移数等于相应内存区域中块或页与首块或首页的偏移数（不适用于家区域）。所以，我们可以使用表项的偏移计算出块或页在内存区域中的硬件地址。 

为了达到第二个目标，内存控制器使用检查点内存区域ID来确定一个检查点写的目的内存区域。我们采用和上文相同的方法，通过表项的偏移数来确定硬件地址。 

为了达到第三个目标，内存控制器依据BTT和PTT 的写计数来确定数据的空间局部性。BTT/PTT中的每个写计数器记录了上一个时间单元中对相应物理块/页的写操作的数据；内存控制器在每个时间单元开始时读取并重置写计数器。一个时间单元内对同一个物理页较大数量的写操作（超过预定义的阈值）意味着这个页有较高的空间局部性和写强度。为此，我们下一个时间单元即采用页回写机制来操作对该页的写。写计数器的值如果小于预定义的阈值，表明我们下一个时间单元内应采用块重映射机制。我们实验了不同的阈值，发现从块重映射切换到页回写以及相反切换的阈值分别取22和16时最为有效。
 
\textbf{生成BTT和PTT的检查点：}元数据定位最近的一致的检查点，所以需要在系统故障前后保持持久化。在每个检查点生成阶段，我们在NVM的BTT和PTT备份区域生成BTT和PTT表项的检查点。有一个挑战在于原子性地将BTT/PTT写入NVM。我们使用一个表征BTT/PTT检查点生成是否完成的比特位来维护备份操作的原子性。 

\textbf{BTT和PTT的大小：}BTT表项的数目取决于工作集合的写强度，而PTT表项的数目由DRAM的大小决定。如果有些负载要求GB级的DRAM，巨大的PTT存储代价可以通过虚拟化PTT得到缓解，即在DRAM存储PTT而在内存控制器中缓存部分热点表项~\cite{Meza2012, Yoon2010, Burcea2008}。图\ref{fig:sens-att-len}显示了我们对不同表长度下系统性能进行的敏感度测试。另外，当前硬件的发展和创新使得内存控制器可以拥有复杂的功能，例如，HMC~\cite{HMC:2011}的逻辑层，或者HP的The Machine~\cite{TheMachine:2015}的媒介控制器。这些技术让我们的硬件开销变得可行。

\subsection{服务读写请求}

为了服务一个内存读请求，我们用该请求的物理地址在PTT和BTT中查找。如果该物理地址在两张表中都没有，那么对应的可见数据位于NVM的家区域中。为了服务一个内存写请求，如图~\ref{fig:flow}（a）所示，我们首先使用PTT来确定是使用块重映射模式还是页回写模式——我们对所有PTT不包含的请求采用块重映射，不论它是否包含在BTT中。如果两张表中都不包含，说明对应的数据块位于家区域；这种情况下，我们需要在BTT中添加一个表项，记录该请求带来的元数据更新。对于BTT/PTT溢出，即没有表项空或者可以被替换的情况，我们只得生成一个检查点并开始一个新的时间单元。这样，我们可以安全地将那些表明其最新检查点在家区域的表项清除。

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{flow}
\caption{流程图：\textbf{（a）}在某个时间单元的执行阶段服务一个写请求；\textbf{（b）}在该时间单元的检查点生成阶段生成检查点。为了显示（a）中写入的数据和（b）中检查点数据的关联关系，我们用三个背景方格标记出两种检查点生成模式和它们之间的合作：每个背景方格内，在（a）中存储的数据，在生成检查点的时候，会经历（b）中对应的处理过程。}
\label{fig:flow}
\end{figure}

\textbf{检查点生成：}当我们生成内存数据的检查点时，我们确保元数据在数据之后生成检查点。特别的，我们采用如下的生成检查点的顺序：（1）将缓冲在DRAM中的$W^{block}_{active}$写入NVM；（2）在NVM生成BTT的检查点；（3）将DRAM中的$W^{page}_{active}$写入NVM；（4）在NVM生成PTT的检查点。

\textbf{接收初始的数据访问：}初始地，当系统开始执行，两个地址转换表都是空的；所有可见数据均位于NVM的家区域。因为PTT是空的，我们在第一个时间单元内仅采用块重映射来操作数据更新。之后，ThyNVM将空间局部性高的数据迁移到页回写模式。系统第一次进入稳定状态（即在两种检查点生成模式间迁移的页数下降到一个相对小而稳定的数值）所需要的时间依赖于PTT的大小以及负载的内存访问行为。我们观察到，实验中使用的负载（第\ref{sec:thnvm-eval}节）的最坏情况，可能需要几十个时间单元使2048个PTT表项达到这种稳态。

\subsection{数据冲刷}

为了保证检查点的一致性，在每个时间单元的生成检查点阶段，我们需要将处理器中的数据冲刷到NVM。为此，我们在每个检查点生成阶段的起始，将处理器寄存器保存到NVM，并且冲刷写缓冲和脏数据块；我们还需要在每个检查点生成阶段结束时，在NVM写队列中插入一个分界。 
我们采用基于硬件的机制来执行数据冲刷。内存控制器在一个执行阶段结束时，通过我们添加的信号路径通知CPU。在收到通知后，CPU发出一个冲刷指令将所有内存器的值写入一个NVM上的特别区域，并将写缓存及脏缓存块写入内存控制器。注意我们并不使缓存中的脏数据块失效，类似于Intel的CLWB操作，从而后续的时间单元可以继续访问这些脏数据块。

\subsection{数据恢复} 
对于采用ThyNVM维护故障时数据一致性的系统，数据恢复涉及三个步骤使持久化内存回滚到最新的检查点。首先，我们将BTT和PTT重新载入内存控制器，把它们对应于可见数据版本（定义见第\ref{subsec:thnvm-space}节）的备份从NVM拷贝出来。照此，我们恢复了可以用于恢复数据的元数据信息。第二，我们需要恢复页回写模式管理的可见数据。因为这些页的可见数据版本应当被保存在DRAM中，我们需要将NVM中的检查点拷贝到DRAM中。当元数据和数据都恢复完毕后，我们执行最后一步，从NVM的专属空间重新载入CPU寄存器。 

一旦内存数据和处理器状态被恢复，我们即继续CPU的执行。然而，因为我们并不恢复外部设备（如网卡）的状态，恢复的程序很有可能遇到由于丢失这些设备状态而导致的错误。在我们的故障模型中，系统故障事件以设备错误的形式对软件程序可见。我们依赖软件固有的异常或错误处理机制来应对这些设备错误。在第\ref{sec:thnvm-discuss}节有关于外部状态（如网卡中的状态）丢失的讨论。 

\section{系统测评}
\label{sec:thnvm-eval}

\subsection{实验环境}

本节描述我们使用的模拟器、处理器和内存配置，以及标准测试集。我们的实验基于周期精准的模拟器gem5~\cite{Binkert:2011:GS:2024716.2024718}。我们使用gem5的提供详细时间模型的处理器，并扩展原有经典内存模型~\cite{6844484}。表\ref{table:config}列举了我们模拟用处理器和内存的架构配置和主要详细参数。我们的DRAM和NVM都是采用DDR3接口进行模拟。我们模拟了16~MB的DRAM，以及分别2048个BTT和PTT表项。我们更改了gem5的实现来加入在第\ref{sec:implementation}节描述的ThyNVM的各项机制。每个时间单元的长度被限制在10~ms（与论文~\cite{1003567, 1003568}的配置相近），因为用户或系统软件总是希望系统故障时的数据损失最小。 

\begin{table}[!h]
\centering
\caption{系统配置及参数~\cite{Lee:2009:APC:1555754.1555758}。}
\label{table:config}
\begin{tabular}{l|l}
\toprule[1.5pt]
处理器 & $3~GHz$，按序执行 \\
\hline
L1指令/数据缓存 & 私有$32~KB$，$8$路，$64~B$块；$4$周期命中延迟 \\
L2缓存 & 私有$256~KB$，$8$路，$64~B$块；$12$周期命中延迟 \\
L3缓存 & 共享$2~MB$每核，$16$路，$64~B$块；$28$周期命中延迟 \\
\hline
内存 & DDR3-1600，$64$位内存通道，$2$ ranks，$8$ banks，$8~KB$行缓存 \\
时间参数 & DRAM：$40$ ($80$)纳秒行命中（错失）延迟 \\
               & NVM：$40$ ($128$/$368$)纳秒行命中（clean/dirty miss）延迟 \\
               & BTT/PTT：$3$纳秒查找延迟 \\
\toprule[1.5pt]
\end{tabular}
\end{table}

\textbf{参评系统}。我们比较ThyNVM和如下四个不同的系统。
\begin{enumerate}
\item 理想化的DRAM系统：一个只有DRAM作为主存的系统，并假设不需要任何代价即可实现数据一致性。DRAM的大小等于ThyNVM的物理地址空间大小。 
\item 理想化的NVM系统：一个只有NVM作为主存的系统，并假设不需要任何代价即可实现数据一致性。NVM的大小等于ThyNVM的物理地址空间大小。 
\item 日志系统：一个混合的NVM系统，使用日志~\cite{DeWitt:1984:ITM:602259.602261, Hagmann:1987:RCF:41457.37518,
ext4}来保证数据的一致性。我们的实现遵照~\cite{Remzi:Journaling}。DRAM中设置一个日志缓冲来收集和归并更新的数据块。每个时间单元结束时，缓冲数据被回写到NVM的备份区域，然后再在原目标地址更新数据。该机制使用一个表来追踪DRAM中被缓冲的脏数据块。这个表的长度与ThyNVM中BTT和PTT两个表的长队和相同。 
\item 影子页系统：一个混合的NVM，使用影子页机制~\cite{bernstein2009principles}来保证数据一致性。该机制对NVM实行写时拷贝，并在DRAM中保存这些页作为缓冲。当DRAM缓冲被填满时，脏页被写入NVM，但不会覆盖原目标地址的数据。该配置中DRAM的大小和ThyNVM中的DRAM大小一样。
\end{enumerate}
 
\textbf{标准测试集}。我们评测了三组不同的负载。

\begin{enumerate}
\item 微型标准测试集涵盖不同的内存访问模式。为了显示我们一致性机制对不用访问模式的适应性，我们评测了三组代表典型访问模式的微型标准测试集。（i）随机：随机访问一个大数组。（ii）流式：顺序访问一个大数组。（iii）滑动：在大数组上虚拟一个工作数据集并滑动。每一步，随机访问数组中的一段特定区域，然后转移到另一个区域。上述每个负载都包含1:1的读写操作数目。
\item 面向存储的内存中标准测试集。持久性内存可通过将传统存储的数据纺织在内存来优化基于磁盘存储的应用。为了评测ThyNVM对此类应用的影响，我们运行了两个标准测试集（类似于~\cite{Coburn:2011:NMP:1950365.1950380, Zhao:2013:KCP:2540708.2540744}）。它们包含代表典型内存存储的键值存储的实现，分别在哈希表和红黑树的内存数据结构上执行查找、插入和删除等操作。 
\item SPEC CPU 2006。 因为ThyNVM为支持多种类型的遗留代码设计，所以我们也评测了SPEC标准测试集。承担此类负载的进程因为可以假定一个可持久化的有一致性保障的地址空间而获益。我们从SPEC套件中选择了8个内存访问最集中的应用，每个应用执行10亿条指令。对于其他内存访问不太集中的应用，ThyNVM的影响可以忽略不计，几乎等同于提供零代价一致性保证的全DRAM理想系统。 
\end{enumerate}

\subsection{微型标准测试集}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\linewidth]{time}
  \caption{微型标准测试集执行时间。}
  \label{fig:micro-time}
\end{figure}

\textbf{整体性能：}图~\ref{fig:micro-time}展示了各参测系统与零代价一致性的全DRAM理想系统的程序运行时间比。通过这些结果，我们可以得出三点观察。第一，ThyNVM的性能在微型标准测试集的所有访问模式下均优于其他一致性机制。我们的性能平均比日志和影子页技术分别高10.2\%和14.8\%。第二，与以前一致性机制不同，ThyNVM可以灵活地适应不同的访问模式。例如，影子页机制在随即访问的模式下性能非常糟糕，因为即使内存中一个页只有很少的脏块，整个页都需要在生成检查点时写入NVM。另一方面，影子页在其他两类访问模式下表现要好于日志机制，因为这些负载下有大量的顺序写被DRAM吸收，减小了检查点生成的代价。然而，ThyNVM可以适应不同的访问模式，没有任何表现不良的情况。第三，ThyNVM的平均性能损失可以限制在理想DRAM系统的14.3\%，整体性能比理想NVM系统高出5.9\%。

总结来看，我们可以得出两个结论：
\begin{enumerate}
\item ThyNVM可以灵活地适应不同的访问模式，其性能在所有微标准测试中高于传统日志或影子页一致性机制。 
\item ThyNVM引入了可以接受的较小的性能损耗以达成数据一致性保障，其性能和零代价一致性的理想系统相差无几。
\end{enumerate}

\textbf{NVM写入量}。图\ref{fig:rand-bw}到图\ref{fig:slid-bw}展示了各参评系统在不同负载下的NVM写入数据的总量。我们分析了三个组件：（1）最后一级缓存的回写，代表CPU向NVM输出的数据量；（2）检查点生成带来的NVM数据写入量；（3）页迁移带来的NVM数据写入量。我们可以得到关于数据传输的三点观察。
\begin{enumerate}
\item 日志和影子页无法自适应于不同的访问模式，导致在至少一种负载下的巨大的NVM传输量。相比之下，ThyNVM在任意负载下都没有显示极端的NVM传输量。平均来看，ThyNVM的NVM写入量，相比日志和影子页，分别降低了10.8\%和14.4\%。
\item 虽然平均来看，ThyNVM虽然减少了对所有访问模式平均的NVM写入量，它比单个访问模式下拥有最低NVM写入量的一致性机制会传输更多的数据。这个原因主要在于ThyNVM将检查点生成和程序执行重叠，需要存储更多的数据版本，有些情况下需要为检查点生成更多的数据。所以，在一致性机制中有一个性能和带宽之间的清晰的折衷。
\item 我们发现，对于不同的访问模式，ThyNVM的页迁移可能产生不同的NVM写入量。例如，流式访问模式下，页不断地从NVM迁移到DRAM再被写回NVM而没有实质上的数据归并，所以产生的NVM写入量很大。与之相反，在滑动窗口访问模式下，因为工作数据集缓慢移动，许多页在被迁移回NVM前可以得到多次重复使用。 
\end{enumerate}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{rand-bw}\\
\caption{微型标准测试集随机访问模式产生的NVM写入量和检查点生成延迟（\%）。``CPU''表示从CPU写入NVM的数据量。}
\label{fig:rand-bw}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{strm-bw}\\
\caption{微型标准测试集流式访问模式产生的NVM写入量和检查点生成延迟（\%）。``CPU''表示从CPU写入NVM的数据量。}
\label{fig:strm-bw}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{slid-bw}\\
\caption{微型标准测试集滑动访问模式产生的NVM写入量和检查点生成延迟（\%）。``CPU''表示从CPU写入NVM的数据量。}
\label{fig:slid-bw}
\end{figure}

总结起来，我们得出如下两个结论。
\begin{enumerate}
\item ThyNVM对不同访问模式的适应性减少了各个访问模式下平均的NVM写入量。 
\item 通过将检查点生成和程序执行重合，ThyNVM可以减少程序执行时间，但对于特定的单一类型的负载，可能会引入比对应的最优传统一致性机制更多的NVM写入量。
\end{enumerate}

\textbf{检查点生成延迟}。图\ref{fig:rand-bw}到图\ref{fig:slid-bw}同时展现了每个负载在检查点生成上花费了多大比例的运行时间。日志和影子页分别花费了平均18.9\%和15.2\%的时间用于生成检查点，而ThyNVM将该比例降低到平均2.5\%。我们可以得出结论，ThyNVM可以通过将检查点生成和程序执行重合有效地避免给应用带来的停滞时间。 

\subsection{面向存储的标准测试集}

内存存储应用通常访问数据结构的随机位置，连续的写较少。我们通过面向存储的标准测试集获得的实验结果，反映了ThyNVM在真实负载情况下的表现。

\textbf{吞吐率性能：}图\ref{fig:ht-thr}和图\ref{fig:rb-thr}展示了两个内存存储负载的事务吞吐率。图中，我们将发向两个键值存储的请求的大小由16B变化到4KB。我们从该图可以做出两个观察。
\begin{enumerate}
\item ThyNVM的吞吐率总是好于传统一致性机制。总体上，ThyNVM在哈希表和红黑树上实现了比日志机制分别高8.8\%和4.3\%的吞吐率，以及比影子页高29.9\%和43.1\%的吞吐率。
\item ThyNVM实现了和理想化DRAM系统接近的吞吐率，以及和理想化NVM系统相似的吞吐率。ThyNVM在哈希表和红黑树上的平均吞吐率分别是理想化DRAM系统的95.1\%和96.2\%。
\end{enumerate}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\linewidth]{ht-thr}\\
  \caption{基于哈希表的键值存储的事务吞吐率。}
  \label{fig:ht-thr}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\linewidth]{rb-thr}\\
  \caption{基于红黑树的减值存储的事务吞吐率。}
  \label{fig:rb-thr}
\end{figure}

总结起来，ThyNVM在实际的内存存储负载下，比传统一致性机制性能更优，而且相对理想化的DRAM/NVM系统造成的吞吐率降低很小。 

\textbf{内存带宽消耗：}图\ref{fig:ht-bw}和图\ref{fig:rb-bw}展示了两个内存存储负载在不同请求大小情况下的带宽消耗。通过该图我们可以得出两个观察。
\begin{enumerate}
\item ThyNVM总比影子页机制占用更少的带宽，因为这些负载展现出随机写的行为。这种情况下，影子页机制由于需要写回只有少数脏数据块的页，生成检查点的带宽会增加。ThyNVM在哈希表和红黑树上的带宽消耗分别比影子页机制减少43.4\%和64.2\%。
\item ThyNVM在这些负载下，会比日志机制带来更多的带宽消耗。原因是ThyNVM需要为了将检查点生成和程序执行重叠而维护更多的数据版本。这个结果符合我们之前关于带宽消耗和性能之间折衷的观察。日志机制在哈希表和红黑树的负载下，分别比ThyNVM减少19.0\%和14.0\%的NVM写入带宽。
\end{enumerate}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{ht-bw}\\
\caption{基于哈希表的键值存储的写带宽消耗。``理想化DRAM''对应DRAM的写入带宽，其余对应NVM写入带宽。}
\label{fig:ht-bw}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\linewidth]{rb-bw}\\
\caption{基于红黑树的键值存储的写带宽消耗。``理想化DRAM''对应DRAM的写入带宽，其余对应NVM写入带宽。}
\label{fig:rb-bw}
\end{figure}

我们可以看到，ThyNVM的写带宽消耗接近日志机制，而远小于影子页机制。

\subsection{面向计算的标准测试集}

图\ref{fig:cpu-ipc}显示内存使用集中的SPEC CPU标准测试集负载的IPC。这些数据根据理想化的DRAM系统进行了统一化。从图中我们可以做出两点观察。第一，ThyNVM相对于理想化的DRAM系统减缓标准测试集执行3.4\%。第二，ThyNVM相对于理想化的NVM系统平均加速标准测试集2.7\%。总结起来，ThyNVM可以显著减少检查点生成的性能损耗，提供和零代价一致性系统相近的性能。
 
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\linewidth]{cpu-ipc}
  \caption{SPEC 2006标准测试集的IPC（数值经过统一化）。}
  \label{fig:cpu-ipc}
\end{figure}
 
\subsection{敏感度分析}

图\ref{fig:sens-att-len}展示了ThyNVM在运行基于哈希表的内存存储负载时，对于BTT表项数目的敏感度。我们可以从图中得到两点结论。第一，基本上BTT越大，NVM写入量越小，因为减小的检查点生成次数相应减小了到NVM的写。第二，基本上BTT越大，负载吞吐率越高。这是因为较高的内存带宽消耗和较小的BTT组合在一起时，会阻塞内存带宽，导致服务内存请求的延迟。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.9\linewidth]{sens-att-len}
  \caption{在面向存储的标准测试集的负载下变化BTT表长度。}
  \label{fig:sens-att-len}
\end{figure}

\section{讨论}
\label{sec:thnvm-discuss}

\section{本章小结}


